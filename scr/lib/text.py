import re
import unicodedata

def normalize(text: str, *, casefold: bool = True, yo2e: bool = True) -> str:
    result = text
    if yo2e:
        result = result.replace('Ñ‘', 'Ðµ').replace('Ð', 'Ð•') # Ð·Ð°Ð¼ÐµÐ½Ð° Ñ‘/Ðµ, Ð/Ð•
    if casefold:
        result = result.casefold() # Ð½Ð¸Ð¶Ð½Ð¸Ð¹ Ñ€ÐµÐ³Ð¸ÑÑ‚Ñ€
    result = ''.join(char if not unicodedata.category(char).startswith('C') or char == '\n' else ' ' for char in result) # Ð·Ð°Ð¼ÐµÐ½Ð° ÑƒÐ¿Ñ€Ð°Ð²Ð»ÑÑŽÑ‰Ð¸Ñ… ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð² Ð½Ð° Ð¿Ñ€Ð¾Ð±ÐµÐ»Ñ‹
    result = re.sub(r'\s+', ' ', result) # ÑÐºÐ»ÐµÐ¸Ð²Ð°Ð½Ð¸Ðµ Ð¿Ð¾Ð²Ñ‚Ð¾Ñ€ÑÑŽÑ‰Ð¸Ñ…ÑÑ Ð¿Ñ€Ð¾Ð±ÐµÐ»Ð¾Ð²
    return result.strip() # ÑƒÐ´Ð°Ð»ÐµÐ½Ð¸Ðµ Ð¿Ñ€Ð¾Ð±ÐµÐ»Ð¾Ð² Ð² Ð½Ð°Ñ‡Ð°Ð»Ðµ Ð¸ Ð² ÐºÐ¾Ð½Ñ†Ðµ

def tokenize(text: str) -> list[str]:
    pattern = r'\w+(?:-\w+)*' # Ð²Ñ‹Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ Ð´Ð»Ñ Ð¿Ð¾Ð¸ÑÐºÐ° ÑÐ»Ð¾Ð²
    tokens = re.findall(pattern, text) # Ð¿Ð¾Ð¸ÑÐº ÑÐ¾Ð²Ð¿Ð°Ð´ÐµÐ½Ð¸Ð¹ Ñ Ð¿Ð°Ñ‚ÐµÑ€Ð½Ð¾Ð¼
    return tokens

def count_freq(tokens: list[str]) -> dict[str, int]:
    freq = {}
    for token in tokens:
        freq[token] = freq.get(token, 0) + 1 # ÑƒÐ²ÐµÐ»Ð¸Ñ‡ÐµÐ½Ð¸Ðµ ÑÑ‡ÐµÑ‚Ñ‡Ð¸ÐºÐ° Ð´Ð»Ñ Ñ‚ÐµÐºÑƒÑ‰ÐµÐ³Ð¾ Ñ‚Ð¾ÐºÐµÐ½Ð° (ÐµÑÐ»Ð¸ Ñ‚Ð¾ÐºÐµÐ½Ð° Ð½ÐµÑ‚ Ð² ÑÐ»Ð¾Ð²Ð°Ñ€Ðµ - get Ð²ÐµÑ€Ð½ÐµÑ‚ 0)
    return freq

def top_n(freq: dict[str, int], n: int = 5) -> list[tuple[str, int]]:
    sorted_items = sorted(freq.items(), 
                         key=lambda x: (-x[1], x[0])) # ÑÐ¾Ñ€Ñ‚Ð¸Ñ€Ð¾Ð²ÐºÐ° ÑÐ»Ð¾Ð²Ð°Ñ€Ñ Ð¿Ð¾ ÑƒÐ±Ñ‹Ð²Ð°Ð½Ð¸ÑŽ Ñ‡Ð°ÑÑ‚Ð¾Ñ‚Ñ‹ (Ð¿Ñ€Ð¸ Ñ€Ð°Ð²ÐµÐ½ÑÑ‚Ð²Ðµ - Ð¿Ð¾ Ð°Ð»Ñ„Ð°Ð²Ð¸Ñ‚Ñƒ)
    return sorted_items[:n] # Ð²Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÐ¼ Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ðµ n ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚Ð¾Ð²

# Ð¢ÐµÑÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¹
# if __name__ == "__main__":
#     print("Ð¢ÐµÑÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ normalize:")
    
#     result = normalize("ÐŸÑ€Ð˜Ð²Ð•Ñ‚\nÐœÐ˜Ñ€\t")
#     print(f"'ÐŸÑ€Ð˜Ð²Ð•Ñ‚\\nÐœÐ˜Ñ€\\t' -> '{result}'")
    
#     result = normalize("Ñ‘Ð¶Ð¸Ðº, ÐÐ»ÐºÐ°", yo2e=True)
#     print(f"'Ñ‘Ð¶Ð¸Ðº, ÐÐ»ÐºÐ°' -> '{result}'")
    
#     result = normalize("Hello\r\nWorld")
#     print(f"'Hello\\r\\nWorld' -> '{result}'")
    
#     result = normalize("  Ð´Ð²Ð¾Ð¹Ð½Ñ‹Ðµ   Ð¿Ñ€Ð¾Ð±ÐµÐ»Ñ‹  ")
#     print(f"'  Ð´Ð²Ð¾Ð¹Ð½Ñ‹Ðµ   Ð¿Ñ€Ð¾Ð±ÐµÐ»Ñ‹  ' -> '{result}'")
    
#     print("\nÐ¢ÐµÑÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ tokenize:")
    
#     result = tokenize("Ð¿Ñ€Ð¸Ð²ÐµÑ‚ Ð¼Ð¸Ñ€")
#     print(f"'Ð¿Ñ€Ð¸Ð²ÐµÑ‚ Ð¼Ð¸Ñ€' -> {result}")
    
#     result = tokenize("hello,world!!!")
#     print(f"'hello,world!!!' -> {result}")
    
#     result = tokenize("Ð¿Ð¾-Ð½Ð°ÑÑ‚Ð¾ÑÑ‰ÐµÐ¼Ñƒ ÐºÑ€ÑƒÑ‚Ð¾")
#     print(f"'Ð¿Ð¾-Ð½Ð°ÑÑ‚Ð¾ÑÑ‰ÐµÐ¼Ñƒ ÐºÑ€ÑƒÑ‚Ð¾' -> {result}")
    
#     result = tokenize("2025 Ð³Ð¾Ð´")
#     print(f"'2025 Ð³Ð¾Ð´' -> {result}")
    
#     result = tokenize("emoji ðŸ˜€ Ð½Ðµ ÑÐ»Ð¾Ð²Ð¾")
#     print(f"'emoji ðŸ˜€ Ð½Ðµ ÑÐ»Ð¾Ð²Ð¾' -> {result}")
    
#     print("\nÐ¢ÐµÑÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ count_freq + top_n:")
    
#     tokens = ["a", "b", "a", "c", "b", "a"]
#     freq = count_freq(tokens)
#     top = top_n(freq, 2)
#     print(f"{tokens} -> {freq} -> {top}")
    
#     tokens = ["bb", "aa", "bb", "aa", "cc"]
#     freq = count_freq(tokens)
#     top = top_n(freq, 2)
#     print(f"{tokens} -> {freq} -> {top}")